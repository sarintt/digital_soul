# Digital Soul ðŸ§ ðŸ”Š

**A Multi-Modal AI Approach to Personality Prediction from Speech**

Digital Soul is a deep learning system that predicts a speakerâ€™s **Big Five personality traits**  
(Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) from short voice clips.

The model fuses:

- **Linguistic analysis** (what you say â€“ ModernBERT)
- **Acoustic analysis** (how you say it â€“ MFCC-based prosody features)

to build a holistic *"personality fingerprint"*.

## ðŸŒŸ Key Features

### âœ” Modern Multi-Modal Architecture
- **Text Encoder:** ModernBERT (CLS embedding, 768-dim)
- **Audio Encoder:** BiLSTM + Attention using **40-dim enriched MFCC features**:
  - MFCC (13)
  - Î” MFCC (13)
  - Î”Â² MFCC (13)
  - RMS energy (1)

### âœ” Advanced Fusion Technique
- **Gated Fusion Layer** learns how to balance linguistic vs acoustic information dynamically.  
  â†’ Fusion performs **better than text-only for the first time**.

### âœ” Enhanced Training Stability
- AdamW optimizer  
- SmoothL1Loss  
- Gradient clipping  
- ReduceLROnPlateau scheduler  
- Early stopping  
- **Modality dropout** (forces model to learn both modalities)

### âœ” Full Scientific Evaluation
- MAE computation  
- Modality ablation (Text-only, Audio-only, Fusion)
- Scatter plots  
- Latency benchmarking

### âœ” Interactive Demo
- Upload a voice clip
- Real-time transcription + feature extraction
- Visual personality radar chart via Gradio

## ðŸ› ï¸ Installation

This project uses `uv` for ultra-fast dependency management.

### Prerequisites

* **Python 3.11** (Required for Librosa/Numba compatibility)
* **FFmpeg** installed on your system (Required for Whisper audio processing).
  * *Windows:* `winget install -e --id Gyan.FFmpeg`
  * *Mac:* `brew install ffmpeg`
  * *Linux:* `sudo apt install ffmpeg`

### Setup Steps

1. **Clone the repository:**

   ```bash
   git clone [https://github.com/sarinntt/dsi442_2025.git](https://github.com/sarinntt/dsi442_2025.git)
   cd dsi442_2025
   ```

2. **Initialize environment with uv:**

    ```bash
    uv sync --no-install-project
    ```

*This installs PyTorch (GPU enabled), Transformers, Librosa, and all other dependencies.*

## ðŸš€ Usage Pipeline
The entire workflow is managed via the `main.py` CLI.

1. **Data Preparation** (`prep`)
Converts raw video files (`.mp4`) into standardized audio files (`.wav`, 16kHz, Mono).
* *Input:* `data/raw_videos/`
* *Output:* `data/processed_audio/`

    ```bash
    uv run main.py prep
    ```

2. **Feature Extraction** (`extract`)
Extracts mathematical features from the audio.
* **Linguistic**: Transcribes audio (Whisper) -> Tokenizes -> Embeds via ModernBERT (768 dim).
* **Acoustic**: Using Librosa per frame:

| Feature Type | Dim |
|--------------|-----|
| MFCC | 13 |
| Î”-MFCC | 13 |
| Î”Â²-MFCC | 13 |
| RMS Energy | 1 |
| **Total** | **40** |


* *Output:* `.npy` files in `data/features/`

1. **Model Training** (`train`)
Trains the Multi-Modal Neural Network using the extracted features.
* **Config:** AdamW optimizer, SmoothL1Loss, ReduceLROnPlateau scheduler, gradient clipping, modality dropout, early stopping.
* *Output:* Saves the best model to `checkpoints/digital_soul_final.pth.`

    ```bash
    uv run main.py train
    ```

1. **Evaluation** (`evaluate`)
Runs the complete scientific evaluation pipeline, including overall MAE, per-trait MAE, modality ablation (Text-Only, Audio-Only, Full Fusion), scatter plots, and latency benchmarking. All evaluation charts are saved in `results/`.

    ```bash
    uv run evaluate.py
    ```

## ðŸŽ® Interactive Demo
Launch the web interface to test the model with your own voice.

    ```bash
    uv run app.py
    ```

* Open the local URL (e.g., `http://127.0.0.1:7860`) in your browser.
* Upload a `.wav` or `.mp3` file.
* View your Personality Radar Chart.

## ðŸ“‚ Project Structure
```
Digital_Soul/
â”œâ”€â”€ app.py                 # Gradio Web Application
â”œâ”€â”€ config.py              # Hyperparameters & Settings
â”œâ”€â”€ main.py                # Command-line Pipeline Controller
â”œâ”€â”€ evaluate.py            # Model Evaluation & Ablation
â”œâ”€â”€ checkpoints/           # Saved Model Weights
â”œâ”€â”€ results/               # Charts & Plots
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ dataset.py         # PyTorch Dataset Loader
    â”œâ”€â”€ features.py        # Whisper + ModernBERT + MFCC40 Extractor
    â”œâ”€â”€ model.py           # BiLSTM + Attention + Gated Fusion Model
    â”œâ”€â”€ preprocessing.py   # Audio Conversion Utilities
    â”œâ”€â”€ trainer.py         # Training Loop (AdamW + SmoothL1Loss)
    â””â”€â”€ utils.py           # Helper Functions

## ðŸ“œ Dataset
This project uses the **ChaLearn First Impressions V2** dataset.
* **Size:** ~10,000 video clips (15s average).
* **Labels:** Big Five Personality Traits (0.0 - 1.0).
*Note: Dataset must be obtained via official challenge channels.*

## ðŸ“„ License
This project is for academic purposes.